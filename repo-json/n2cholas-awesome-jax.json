[
  {
    "name": "Haiku",
    "url": "https://github.com/deepmind/dm-haiku",
    "description": "Focused on simplicity, created by the authors of Sonnet at DeepMind. "
  },
  {
    "name": "Objax",
    "url": "https://github.com/google/objax",
    "description": "Has an object oriented design similar to PyTorch. "
  },
  {
    "name": "Elegy",
    "url": "https://poets-ai.github.io/elegy/",
    "description": "A framework-agnostic Trainer interface for the Jax ecosystem. Supports Flax, Haiku, and Optax. "
  },
  {
    "name": "Trax",
    "url": "https://github.com/google/trax",
    "description": "\"Batteries included\" deep learning library focused on providing solutions for common workloads. "
  },
  {
    "name": "Jraph",
    "url": "https://github.com/deepmind/jraph",
    "description": "Lightweight graph neural network library. "
  },
  {
    "name": "Neural Tangents",
    "url": "https://github.com/google/neural-tangents",
    "description": "High-level API for specifying neural networks of both finite and infinite width. "
  },
  {
    "name": "HuggingFace",
    "url": "https://github.com/huggingface/transformers",
    "description": "Ecosystem of pretrained Transformers for a wide range of natural language tasks (Flax). "
  },
  {
    "name": "NumPyro",
    "url": "https://github.com/pyro-ppl/numpyro",
    "description": "Probabilistic programming based on the Pyro library. "
  },
  {
    "name": "Chex",
    "url": "https://github.com/deepmind/chex",
    "description": "Utilities to write and test reliable JAX code. "
  },
  {
    "name": "Optax",
    "url": "https://github.com/deepmind/optax",
    "description": "Gradient processing and optimization library. "
  },
  {
    "name": "RLax",
    "url": "https://github.com/deepmind/rlax",
    "description": "Library for implementing reinforcement learning agents. "
  },
  {
    "name": "JAX, M.D.",
    "url": "https://github.com/google/jax-md",
    "description": "Accelerated, differential molecular dynamics. "
  },
  {
    "name": "Coax",
    "url": "https://github.com/coax-dev/coax",
    "description": "Turn RL papers into code, the easy way. "
  },
  {
    "name": "SymJAX",
    "url": "https://github.com/SymJAX/SymJAX",
    "description": "Symbolic CPU/GPU/TPU programming. "
  },
  {
    "name": "mcx",
    "url": "https://github.com/rlouf/mcx",
    "description": "Express & compile probabilistic programs for performant inference. "
  },
  {
    "name": "Distrax",
    "url": "https://github.com/deepmind/distrax",
    "description": "Reimplementation of TensorFlow Probability, containing probability distributions and bijectors. "
  },
  {
    "name": "cvxpylayers",
    "url": "https://github.com/cvxgrp/cvxpylayers",
    "description": "Construct differentiable convex optimization layers. "
  },
  {
    "name": "TensorLy",
    "url": "https://github.com/tensorly/tensorly",
    "description": "Tensor learning made simple. "
  },
  {
    "name": "Equivariant MLP",
    "url": "https://github.com/mfinzi/equivariant-MLP",
    "description": "Construct equivariant neural network layers. "
  },
  {
    "name": "jax-resnet",
    "url": "https://github.com/n2cholas/jax-resnet/",
    "description": "Implementations and checkpoints for ResNet variants in Flax. "
  },
  {
    "name": "jax-unirep",
    "url": "https://github.com/ElArkk/jax-unirep",
    "description": "Library implementing the UniRep model for protein machine learning applications. "
  },
  {
    "name": "jax-flows",
    "url": "https://github.com/ChrisWaites/jax-flows",
    "description": "Normalizing flows in JAX. "
  },
  {
    "name": "sklearn-jax-kernels",
    "url": "https://github.com/ExpectationMax/sklearn-jax-kernels",
    "description": "scikit-learn kernel matrices using JAX. "
  },
  {
    "name": "jax-cosmo",
    "url": "https://github.com/DifferentiableUniverseInitiative/jax_cosmo",
    "description": "Differentiable cosmology library. "
  },
  {
    "name": "efax",
    "url": "https://github.com/NeilGirdhar/efax",
    "description": "Exponential Families in JAX. "
  },
  {
    "name": "mpi4jax",
    "url": "https://github.com/PhilipVinc/mpi4jax",
    "description": "Combine MPI operations with your Jax code on CPUs and GPUs. "
  },
  {
    "name": "imax",
    "url": "https://github.com/4rtemi5/imax",
    "description": "Image augmentations and transformations. "
  },
  {
    "name": "FlaxVision",
    "url": "https://github.com/rolandgvc/flaxvision",
    "description": "Flax version of TorchVision. "
  },
  {
    "name": "Oryx",
    "url": "https://github.com/tensorflow/probability/tree/master/spinoffs/oryx",
    "description": "Probabilistic programming language based on program transformations."
  },
  {
    "name": "Optimal Transport Tools",
    "url": "https://github.com/google-research/ott",
    "description": "Toolbox that bundles utilities to solve optimal transport problems."
  },
  {
    "name": "delta PV",
    "url": "https://github.com/romanodev/deltapv",
    "description": "A photovoltaic simulator with automatic differentation. "
  },
  {
    "name": "jaxlie",
    "url": "https://github.com/brentyi/jaxlie",
    "description": "Lie theory library for rigid body transformations and optimization. "
  },
  {
    "name": "BRAX",
    "url": "https://github.com/google/brax",
    "description": "Differentiable physics engine to simulate environments along with learning algorithms to train agents for these environments. "
  },
  {
    "name": "flaxmodels",
    "url": "https://github.com/matthias-wright/flaxmodels",
    "description": "Pretrained models for Jax/Flax. "
  },
  {
    "name": "CR.Sparse",
    "url": "https://github.com/carnotresearch/cr-sparse",
    "description": "XLA accelerated algorithms for sparse representations and compressive sensing. "
  },
  {
    "name": "exojax",
    "url": "https://github.com/HajimeKawahara/exojax",
    "description": "Automatic differentiable spectrum modeling of exoplanets/brown dwarfs compatible to JAX. "
  },
  {
    "name": "Reformer",
    "url": "https://github.com/google/trax/tree/master/trax/models/reformer",
    "description": "Implementation of the Reformer (efficient transformer) architecture."
  },
  {
    "name": "Vision Transformer",
    "url": "https://github.com/google-research/vision_transformer",
    "description": "Official implementation in Flax of An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale."
  },
  {
    "name": "Fourier Feature Networks",
    "url": "https://github.com/tancik/fourier-feature-networks",
    "description": "Official implementation of Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains."
  },
  {
    "name": "Flax Models",
    "url": "https://github.com/google-research/google-research/tree/master/flax_models",
    "description": "Collection of models and methods implemented in Flax."
  },
  {
    "name": "JaxNeRF",
    "url": "https://github.com/google-research/google-research/tree/master/jaxnerf",
    "description": "Implementation of NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis with multi-device GPU/TPU support."
  },
  {
    "name": "Big Transfer (BiT)",
    "url": "https://github.com/google-research/big_transfer",
    "description": "Implementation of Big Transfer (BiT): General Visual Representation Learning."
  },
  {
    "name": "NuX",
    "url": "https://github.com/Information-Fusion-Lab-Umass/NuX",
    "description": "Normalizing flows with JAX."
  },
  {
    "name": "kalman-jax",
    "url": "https://github.com/AaltoML/kalman-jax",
    "description": "Approximate inference for Markov (i.e., temporal) Gaussian processes using iterated Kalman filtering and smoothing."
  },
  {
    "name": "GPJax",
    "url": "https://github.com/thomaspinder/GPJax",
    "description": "Gaussian processes in JAX."
  },
  {
    "name": "jaxns",
    "url": "https://github.com/Joshuaalbert/jaxns",
    "description": "Nested sampling in JAX."
  },
  {
    "name": "Normalizer-Free Networks",
    "url": "https://github.com/deepmind/deepmind-research/tree/master/nfnets",
    "description": "Official Haiku implementation of NFNets."
  },
  {
    "name": "Distributed Shampoo",
    "url": "https://github.com/google-research/google-research/tree/master/scalable_shampoo",
    "description": "Implementation of Second Order Optimization Made Practical."
  },
  {
    "name": "JAX (Flax) RL",
    "url": "https://github.com/ikostrikov/jax-rl",
    "description": "Implementations of reinforcement learning algorithms."
  },
  {
    "name": "Introduction to JAX",
    "url": "https://youtu.be/0mVmRHMaOJ4",
    "description": "Simple neural network from scratch in JAX."
  },
  {
    "name": "JAX: Accelerated Machine Learning Research | SciPy 2020 | VanderPlas",
    "url": "https://youtu.be/z-WSrQDXkuM",
    "description": "JAX's core design, how it's powering new research, and how you can start using it."
  },
  {
    "name": "Bayesian Programming with JAX + NumPyro â€” Andy Kitchen",
    "url": "https://youtu.be/CecuWGpoztw",
    "description": "Introduction to Bayesian modelling using NumPyro."
  },
  {
    "name": "JAX: Accelerated machine-learning research via composable function transformations in Python | NeurIPS 2019 | Skye Wanderman-Milne",
    "url": "https://slideslive.com/38923687/jax-accelerated-machinelearning-research-via-composable-function-transformations-in-python",
    "description": "JAX intro presentation in Program Transformations for Machine Learning workshop."
  },
  {
    "name": "JAX on Cloud TPUs | NeurIPS 2020 | Skye Wanderman-Milne and James Bradbury",
    "url": "https://drive.google.com/file/d/1jKxefZT1xJDUxMman6qrQVed7vWI0MIn/edit",
    "description": "Presentation of TPU host access with demo."
  },
  {
    "name": "Deep Implicit Layers",
    "url": "https://slideslive.com/38935810/deep-implicit-layers-neural-odes-equilibrium-models-and-beyond",
    "description": "Neural ODEs, Deep Equilibirum Models, and Beyond | NeurIPS 2020 - Tutorial created by Zico Kolter, David Duvenaud, and Matt Johnson with Colab notebooks avaliable in Deep Implicit Layers."
  },
  {
    "name": "Solving y=mx+b with Jax on a TPU Pod slice",
    "url": "http://matpalm.com/blog/ymxb_pod_slice/",
    "description": "Mat Kelcey - A four part YouTube tutorial series with Colab notebooks that starts with Jax fundamentals and moves up to training with a data parallel approach on a v3-32 TPU Pod slice."
  },
  {
    "name": "Compiling machine learning programs via high-level tracing. Roy Frostig, Matthew James Johnson, Chris Leary. MLSys 2018.",
    "url": "https://mlsys.org/Conferences/doc/2018/146.pdf",
    "description": "White paper describing an early version of JAX, detailing how computation is traced and compiled."
  },
  {
    "name": "JAX, M.D.: A Framework for Differentiable Physics. Samuel S. Schoenholz, Ekin D. Cubuk. NeurIPS 2020.",
    "url": "https://arxiv.org/abs/1912.04232",
    "description": "Introduces JAX, M.D., a differentiable physics library which includes simulation environments, interaction potentials, neural networks, and more."
  },
  {
    "name": "Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization. Pranav Subramani, Nicholas Vadivelu, Gautam Kamath. arXiv 2020.",
    "url": "https://arxiv.org/abs/2010.09063",
    "description": "Uses JAX's JIT and VMAP to achieve faster differentially private than existing libraries."
  },
  {
    "name": "Getting started with JAX (MLPs, CNNs & RNNs) by Robert Lange",
    "url": "https://roberttlange.github.io/posts/2020/03/blog-post-10/",
    "description": "Neural network building blocks from scratch with the basic JAX operators."
  },
  {
    "name": "Tutorial: image classification with JAX and Flax Linen by 8bitmp3",
    "url": "https://github.com/8bitmp3/JAX-Flax-Tutorial-Image-Classification-with-Linen",
    "description": "Learn how to create a simple convolutional network with the Linen API by Flax and train it to recognize handwritten digits."
  },
  {
    "name": "Plugging Into JAX by Nick Doiron",
    "url": "https://medium.com/swlh/plugging-into-jax-16c120ec3302",
    "description": "Compares Flax, Haiku, and Objax on the Kaggle flower classification challenge."
  },
  {
    "name": "Meta-Learning in 50 Lines of JAX by Eric Jang",
    "url": "https://blog.evjang.com/2019/02/maml-jax.html",
    "description": "Introduction to both JAX and Meta-Learning."
  },
  {
    "name": "Normalizing Flows in 100 Lines of JAX by Eric Jang",
    "url": "https://blog.evjang.com/2019/07/nf-jax.html",
    "description": "Concise implementation of RealNVP."
  },
  {
    "name": "Differentiable Path Tracing on the GPU/TPU by Eric Jang",
    "url": "https://blog.evjang.com/2019/11/jaxpt.html",
    "description": "Tutorial on implementing path tracing."
  },
  {
    "name": "Ensemble networks by Mat Kelcey",
    "url": "http://matpalm.com/blog/ensemble_nets",
    "description": "Ensemble nets are a method of representing an ensemble of models as one single logical model."
  },
  {
    "name": "Out of distribution (OOD) detection by Mat Kelcey",
    "url": "http://matpalm.com/blog/ood_using_focal_loss",
    "description": "Implements different methods for OOD detection."
  },
  {
    "name": "Understanding Autodiff with JAX by Srihari Radhakrishna",
    "url": "https://www.radx.in/jax.html",
    "description": "Understand how autodiff works using JAX."
  },
  {
    "name": "From PyTorch to JAX: towards neural net frameworks that purify stateful code by Sabrina J. Mielke",
    "url": "https://sjmielke.com/jax-purify.htm",
    "description": "Showcases how to go from a PyTorch-like style of coding to a more Functional-style of coding."
  },
  {
    "name": "Extending JAX with custom C++ and CUDA code by Dan Foreman-Mackey",
    "url": "https://github.com/dfm/extending-jax",
    "description": "Tutorial demonstrating the infrastructure required to provide custom ops in JAX."
  },
  {
    "name": "Evolving Neural Networks in JAX by Robert Tjarko Lange",
    "url": "https://roberttlange.github.io/posts/2021/02/cma-es-jax/",
    "description": "Explores how JAX can power the next generation of scalable neuroevolution algorithms."
  },
  {
    "name": "Exploring hyperparameter meta-loss landscapes with JAX by Luke Metz",
    "url": "http://lukemetz.com/exploring-hyperparameter-meta-loss-landscapes-with-jax/",
    "description": "Demonstrates how to use JAX to perform inner-loss optimization with SGD and Momentum, outer-loss optimization with gradients, and outer-loss optimization using evolutionary strategies."
  },
  {
    "name": "Deterministic ADVI in JAX by Martin Ingram",
    "url": "https://martiningram.github.io/deterministic-advi/",
    "description": "Walk through of implementing automatic differentiation variational inference (ADVI) easily and cleanly with JAX."
  },
  {
    "name": "Evolved channel selection by Mat Kelcey",
    "url": "http://matpalm.com/blog/evolved_channel_selection/",
    "description": "Trains a classification model robust to different combinations of input channels at different resolutions, then uses a genetic algorithm to decide the best combination for a particular loss."
  },
  {
    "name": "Introduction to JAX by Kevin Murphy",
    "url": "https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/jax_intro.ipynb",
    "description": "Colab that introduces various aspects of the language and applies them to simple ML problems."
  },
  {
    "name": "Writing an MCMC sampler in JAX by Jeremie Coullon",
    "url": "https://www.jeremiecoullon.com/2020/11/10/mcmcjax3ways/",
    "description": "Tutorial on the different ways to write an MCMC sampler in JAX along with speed benchmarks."
  },
  {
    "name": "How to add a progress bar to JAX scans and loops by Jeremie Coullon",
    "url": "https://www.jeremiecoullon.com/2021/01/29/jax_progress_bar/",
    "description": "Tutorial on how to add a progress bar to compiled loops in JAX using the host_callback module."
  },
  {
    "name": "Reddit",
    "url": "https://www.reddit.com/r/JAX/"
  }
]
