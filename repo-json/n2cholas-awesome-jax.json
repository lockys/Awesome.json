[
  {
    "name": "Haiku",
    "url": "https://github.com/deepmind/dm-haiku",
    "description": "Focused on simplicity, created by the authors of Sonnet at DeepMind. "
  },
  {
    "name": "Objax",
    "url": "https://github.com/google/objax",
    "description": "Has an object oriented design similar to PyTorch. "
  },
  {
    "name": "Elegy",
    "url": "https://poets-ai.github.io/elegy/",
    "description": "A High Level API for Deep Learning in JAX. Supports Flax, Haiku, and Optax. "
  },
  {
    "name": "Trax",
    "url": "https://github.com/google/trax",
    "description": "\"Batteries included\" deep learning library focused on providing solutions for common workloads. "
  },
  {
    "name": "Jraph",
    "url": "https://github.com/deepmind/jraph",
    "description": "Lightweight graph neural network library. "
  },
  {
    "name": "Neural Tangents",
    "url": "https://github.com/google/neural-tangents",
    "description": "High-level API for specifying neural networks of both finite and infinite width. "
  },
  {
    "name": "HuggingFace",
    "url": "https://github.com/huggingface/transformers",
    "description": "Ecosystem of pretrained Transformers for a wide range of natural language tasks (Flax). "
  },
  {
    "name": "Equinox",
    "url": "https://github.com/patrick-kidger/equinox",
    "description": "Callable PyTrees and filtered JIT/grad transformations => neural networks in JAX. "
  },
  {
    "name": "NumPyro",
    "url": "https://github.com/pyro-ppl/numpyro",
    "description": "Probabilistic programming based on the Pyro library. "
  },
  {
    "name": "Chex",
    "url": "https://github.com/deepmind/chex",
    "description": "Utilities to write and test reliable JAX code. "
  },
  {
    "name": "Optax",
    "url": "https://github.com/deepmind/optax",
    "description": "Gradient processing and optimization library. "
  },
  {
    "name": "RLax",
    "url": "https://github.com/deepmind/rlax",
    "description": "Library for implementing reinforcement learning agents. "
  },
  {
    "name": "JAX, M.D.",
    "url": "https://github.com/google/jax-md",
    "description": "Accelerated, differential molecular dynamics. "
  },
  {
    "name": "Coax",
    "url": "https://github.com/coax-dev/coax",
    "description": "Turn RL papers into code, the easy way. "
  },
  {
    "name": "Distrax",
    "url": "https://github.com/deepmind/distrax",
    "description": "Reimplementation of TensorFlow Probability, containing probability distributions and bijectors. "
  },
  {
    "name": "cvxpylayers",
    "url": "https://github.com/cvxgrp/cvxpylayers",
    "description": "Construct differentiable convex optimization layers. "
  },
  {
    "name": "TensorLy",
    "url": "https://github.com/tensorly/tensorly",
    "description": "Tensor learning made simple. "
  },
  {
    "name": "NetKet",
    "url": "https://github.com/netket/netket",
    "description": "Machine Learning toolbox for Quantum Physics. "
  },
  {
    "name": "Equivariant MLP",
    "url": "https://github.com/mfinzi/equivariant-MLP",
    "description": "Construct equivariant neural network layers. "
  },
  {
    "name": "jax-resnet",
    "url": "https://github.com/n2cholas/jax-resnet/",
    "description": "Implementations and checkpoints for ResNet variants in Flax. "
  },
  {
    "name": "jax-unirep",
    "url": "https://github.com/ElArkk/jax-unirep",
    "description": "Library implementing the UniRep model for protein machine learning applications. "
  },
  {
    "name": "jax-flows",
    "url": "https://github.com/ChrisWaites/jax-flows",
    "description": "Normalizing flows in JAX. "
  },
  {
    "name": "sklearn-jax-kernels",
    "url": "https://github.com/ExpectationMax/sklearn-jax-kernels",
    "description": "scikit-learn kernel matrices using JAX. "
  },
  {
    "name": "jax-cosmo",
    "url": "https://github.com/DifferentiableUniverseInitiative/jax_cosmo",
    "description": "Differentiable cosmology library. "
  },
  {
    "name": "efax",
    "url": "https://github.com/NeilGirdhar/efax",
    "description": "Exponential Families in JAX. "
  },
  {
    "name": "mpi4jax",
    "url": "https://github.com/PhilipVinc/mpi4jax",
    "description": "Combine MPI operations with your Jax code on CPUs and GPUs. "
  },
  {
    "name": "imax",
    "url": "https://github.com/4rtemi5/imax",
    "description": "Image augmentations and transformations. "
  },
  {
    "name": "FlaxVision",
    "url": "https://github.com/rolandgvc/flaxvision",
    "description": "Flax version of TorchVision. "
  },
  {
    "name": "Oryx",
    "url": "https://github.com/tensorflow/probability/tree/master/spinoffs/oryx",
    "description": "Probabilistic programming language based on program transformations."
  },
  {
    "name": "Optimal Transport Tools",
    "url": "https://github.com/google-research/ott",
    "description": "Toolbox that bundles utilities to solve optimal transport problems."
  },
  {
    "name": "delta PV",
    "url": "https://github.com/romanodev/deltapv",
    "description": "A photovoltaic simulator with automatic differentation. "
  },
  {
    "name": "jaxlie",
    "url": "https://github.com/brentyi/jaxlie",
    "description": "Lie theory library for rigid body transformations and optimization. "
  },
  {
    "name": "BRAX",
    "url": "https://github.com/google/brax",
    "description": "Differentiable physics engine to simulate environments along with learning algorithms to train agents for these environments. "
  },
  {
    "name": "flaxmodels",
    "url": "https://github.com/matthias-wright/flaxmodels",
    "description": "Pretrained models for Jax/Flax. "
  },
  {
    "name": "CR.Sparse",
    "url": "https://github.com/carnotresearch/cr-sparse",
    "description": "XLA accelerated algorithms for sparse representations and compressive sensing. "
  },
  {
    "name": "exojax",
    "url": "https://github.com/HajimeKawahara/exojax",
    "description": "Automatic differentiable spectrum modeling of exoplanets/brown dwarfs compatible to JAX. "
  },
  {
    "name": "JAXopt",
    "url": "https://github.com/google/jaxopt",
    "description": "Hardware accelerated (GPU/TPU), batchable and differentiable optimizers in JAX. "
  },
  {
    "name": "PIX",
    "url": "https://github.com/deepmind/dm_pix",
    "description": "PIX is an image processing library in JAX, for JAX. "
  },
  {
    "name": "bayex",
    "url": "https://github.com/alonfnt/bayex",
    "description": "Bayesian Optimization powered by JAX. "
  },
  {
    "name": "JaxDF",
    "url": "https://github.com/ucl-bug/jaxdf",
    "description": "Framework for differentiable simulators with arbitrary discretizations. "
  },
  {
    "name": "tree-math",
    "url": "https://github.com/google/tree-math",
    "description": "Convert functions that operate on arrays into functions that operate on PyTrees. "
  },
  {
    "name": "jax-models",
    "url": "https://github.com/DarshanDeshpande/jax-models",
    "description": "Implementations of research papers originally without code or code written with frameworks other than JAX. "
  },
  {
    "name": "PGMax",
    "url": "https://github.com/vicariousinc/PGMax",
    "description": "A framework for building discrete Probabilistic Graphical Models (PGM's) and running inference inference on them via JAX. "
  },
  {
    "name": "EvoJAX",
    "url": "https://github.com/google/evojax",
    "description": "Hardware-Accelerated Neuroevolution "
  },
  {
    "name": "evosax",
    "url": "https://github.com/RobertTLange/evosax",
    "description": "JAX-Based Evolution Strategies "
  },
  {
    "name": "SymJAX",
    "url": "https://github.com/SymJAX/SymJAX",
    "description": "Symbolic CPU/GPU/TPU programming. "
  },
  {
    "name": "mcx",
    "url": "https://github.com/rlouf/mcx",
    "description": "Express & compile probabilistic programs for performant inference. "
  },
  {
    "name": "kalman-jax",
    "url": "https://github.com/AaltoML/kalman-jax",
    "description": "Approximate inference for Markov (i.e., temporal) Gaussian processes using iterated Kalman filtering and smoothing."
  },
  {
    "name": "GPJax",
    "url": "https://github.com/thomaspinder/GPJax",
    "description": "Gaussian processes in JAX."
  },
  {
    "name": "jaxns",
    "url": "https://github.com/Joshuaalbert/jaxns",
    "description": "Nested sampling in JAX."
  },
  {
    "name": "Amortized Bayesian Optimization",
    "url": "https://github.com/google-research/google-research/tree/master/amortized_bo",
    "description": "Code related to Amortized Bayesian Optimization over Discrete Spaces."
  },
  {
    "name": "Accurate Quantized Training",
    "url": "https://github.com/google-research/google-research/tree/master/aqt",
    "description": "Tools and libraries for running and analyzing neural network quantization experiments in JAX and Flax."
  },
  {
    "name": "BNN-HMC",
    "url": "https://github.com/google-research/google-research/tree/master/bnn_hmc",
    "description": "Implementation for the paper What Are Bayesian Neural Network Posteriors Really Like?."
  },
  {
    "name": "JAX-DFT",
    "url": "https://github.com/google-research/google-research/tree/master/jax_dft",
    "description": "One-dimensional density functional theory (DFT) in JAX, with implementation of Kohn-Sham equations as regularizer: building prior knowledge into machine-learned physics."
  },
  {
    "name": "Robust Loss",
    "url": "https://github.com/google-research/google-research/tree/master/robust_loss_jax",
    "description": "Reference code for the paper A General and Adaptive Robust Loss Function."
  },
  {
    "name": "Performer",
    "url": "https://github.com/google-research/google-research/tree/master/performer/fast_attention/jax",
    "description": "Flax implementation of the Performer (linear transformer via FAVOR+) architecture."
  },
  {
    "name": "JaxNeRF",
    "url": "https://github.com/google-research/google-research/tree/master/jaxnerf",
    "description": "Implementation of NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis with multi-device GPU/TPU support."
  },
  {
    "name": "Big Transfer (BiT)",
    "url": "https://github.com/google-research/big_transfer",
    "description": "Implementation of Big Transfer (BiT): General Visual Representation Learning."
  },
  {
    "name": "JAX RL",
    "url": "https://github.com/ikostrikov/jax-rl",
    "description": "Implementations of reinforcement learning algorithms."
  },
  {
    "name": "gMLP",
    "url": "https://github.com/SauravMaheshkar/gMLP",
    "description": "Implementation of Pay Attention to MLPs."
  },
  {
    "name": "MLP Mixer",
    "url": "https://github.com/SauravMaheshkar/MLP-Mixer",
    "description": "Minimal implementation of MLP-Mixer: An all-MLP Architecture for Vision."
  },
  {
    "name": "Distributed Shampoo",
    "url": "https://github.com/google-research/google-research/tree/master/scalable_shampoo",
    "description": "Implementation of Second Order Optimization Made Practical."
  },
  {
    "name": "NesT",
    "url": "https://github.com/google-research/nested-transformer",
    "description": "Official implementation of Aggregating Nested Transformers."
  },
  {
    "name": "XMC-GAN",
    "url": "https://github.com/google-research/xmcgan_image_generation",
    "description": "Official implementation of Cross-Modal Contrastive Learning for Text-to-Image Generation."
  },
  {
    "name": "FNet",
    "url": "https://github.com/google-research/google-research/tree/master/f_net",
    "description": "Official implementation of FNet: Mixing Tokens with Fourier Transforms."
  },
  {
    "name": "GFSA",
    "url": "https://github.com/google-research/google-research/tree/master/gfsa",
    "description": "Official implementation of Learning Graph Structure With A Finite-State Automaton Layer."
  },
  {
    "name": "IPA-GNN",
    "url": "https://github.com/google-research/google-research/tree/master/ipagnn",
    "description": "Official implementation of Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks."
  },
  {
    "name": "Flax Models",
    "url": "https://github.com/google-research/google-research/tree/master/flax_models",
    "description": "Collection of models and methods implemented in Flax."
  },
  {
    "name": "Protein LM",
    "url": "https://github.com/google-research/google-research/tree/master/protein_lm",
    "description": "Implements BERT and autoregressive models for proteins, as described in Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences and ProGen: Language Modeling for Protein Generation."
  },
  {
    "name": "Slot Attention",
    "url": "https://github.com/google-research/google-research/tree/master/ptopk_patch_selection",
    "description": "Reference implementation for Differentiable Patch Selection for Image Recognition."
  },
  {
    "name": "Vision Transformer",
    "url": "https://github.com/google-research/vision_transformer",
    "description": "Official implementation of An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale."
  },
  {
    "name": "FID computation",
    "url": "https://github.com/matthias-wright/jax-fid",
    "description": "Port of mseitzer/pytorch-fid to Flax."
  },
  {
    "name": "AlphaFold",
    "url": "https://github.com/deepmind/alphafold",
    "description": "Implementation of the inference pipeline of AlphaFold v2.0, presented in Highly accurate protein structure prediction with AlphaFold."
  },
  {
    "name": "Adversarial Robustness",
    "url": "https://github.com/deepmind/deepmind-research/tree/master/adversarial_robustness",
    "description": "Reference code for Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples and Fixing Data Augmentation to Improve Adversarial Robustness."
  },
  {
    "name": "Bootstrap Your Own Latent",
    "url": "https://github.com/deepmind/deepmind-research/tree/master/byol",
    "description": "Implementation for the paper Bootstrap your own latent: A new approach to self-supervised Learning."
  },
  {
    "name": "Gated Linear Networks",
    "url": "https://github.com/deepmind/deepmind-research/tree/master/gated_linear_networks",
    "description": "GLNs are a family of backpropagation-free neural networks."
  },
  {
    "name": "Glassy Dynamics",
    "url": "https://github.com/deepmind/deepmind-research/tree/master/glassy_dynamics",
    "description": "Open source implementation of the paper Unveiling the predictive power of static structure in glassy systems."
  },
  {
    "name": "MMV",
    "url": "https://github.com/deepmind/deepmind-research/tree/master/mmv",
    "description": "Code for the models in Self-Supervised MultiModal Versatile Networks."
  },
  {
    "name": "Normalizer-Free Networks",
    "url": "https://github.com/deepmind/deepmind-research/tree/master/nfnets",
    "description": "Official Haiku implementation of NFNets."
  },
  {
    "name": "NuX",
    "url": "https://github.com/Information-Fusion-Lab-Umass/NuX",
    "description": "Normalizing flows with JAX."
  },
  {
    "name": "OGB-LSC",
    "url": "https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc",
    "description": "This repository contains DeepMind's entry to the PCQM4M-LSC (quantum chemistry) and MAG240M-LSC (academic graph)"
  },
  {
    "name": "Persistent Evolution Strategies",
    "url": "https://github.com/google-research/google-research/tree/master/persistent_es",
    "description": "Code used for the paper Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies."
  },
  {
    "name": "Two Player Auction Learning",
    "url": "https://github.com/degregat/two-player-auctions",
    "description": "JAX implementation of the paper Auction learning as a two-player game."
  },
  {
    "name": "WikiGraphs",
    "url": "https://github.com/deepmind/deepmind-research/tree/master/wikigraphs",
    "description": "Baseline code to reproduce results in WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Datase."
  },
  {
    "name": "Reformer",
    "url": "https://github.com/google/trax/tree/master/trax/models/reformer",
    "description": "Implementation of the Reformer (efficient transformer) architecture."
  },
  {
    "name": "Introduction to JAX",
    "url": "https://youtu.be/0mVmRHMaOJ4",
    "description": "Simple neural network from scratch in JAX."
  },
  {
    "name": "JAX: Accelerated Machine Learning Research | SciPy 2020 | VanderPlas",
    "url": "https://youtu.be/z-WSrQDXkuM",
    "description": "JAX's core design, how it's powering new research, and how you can start using it."
  },
  {
    "name": "Bayesian Programming with JAX + NumPyro â€” Andy Kitchen",
    "url": "https://youtu.be/CecuWGpoztw",
    "description": "Introduction to Bayesian modelling using NumPyro."
  },
  {
    "name": "JAX: Accelerated machine-learning research via composable function transformations in Python | NeurIPS 2019 | Skye Wanderman-Milne",
    "url": "https://slideslive.com/38923687/jax-accelerated-machinelearning-research-via-composable-function-transformations-in-python",
    "description": "JAX intro presentation in Program Transformations for Machine Learning workshop."
  },
  {
    "name": "JAX on Cloud TPUs | NeurIPS 2020 | Skye Wanderman-Milne and James Bradbury",
    "url": "https://drive.google.com/file/d/1jKxefZT1xJDUxMman6qrQVed7vWI0MIn/edit",
    "description": "Presentation of TPU host access with demo."
  },
  {
    "name": "Deep Implicit Layers",
    "url": "https://slideslive.com/38935810/deep-implicit-layers-neural-odes-equilibrium-models-and-beyond",
    "description": "Neural ODEs, Deep Equilibirum Models, and Beyond | NeurIPS 2020 - Tutorial created by Zico Kolter, David Duvenaud, and Matt Johnson with Colab notebooks avaliable in Deep Implicit Layers."
  },
  {
    "name": "Solving y=mx+b with Jax on a TPU Pod slice",
    "url": "http://matpalm.com/blog/ymxb_pod_slice/",
    "description": "Mat Kelcey - A four part YouTube tutorial series with Colab notebooks that starts with Jax fundamentals and moves up to training with a data parallel approach on a v3-32 TPU Pod slice."
  },
  {
    "name": "JAX, Flax & Transformers ðŸ¤—",
    "url": "https://github.com/huggingface/transformers/blob/9160d81c98854df44b1d543ce5d65a6aa28444a2/examples/research_projects/jax-projects/README.md#talks",
    "description": "3 days of talks around JAX / Flax, Transformers, large-scale language modeling and other great topics."
  },
  {
    "name": "Compiling machine learning programs via high-level tracing. Roy Frostig, Matthew James Johnson, Chris Leary. MLSys 2018.",
    "url": "https://mlsys.org/Conferences/doc/2018/146.pdf",
    "description": "White paper describing an early version of JAX, detailing how computation is traced and compiled."
  },
  {
    "name": "JAX, M.D.: A Framework for Differentiable Physics. Samuel S. Schoenholz, Ekin D. Cubuk. NeurIPS 2020.",
    "url": "https://arxiv.org/abs/1912.04232",
    "description": "Introduces JAX, M.D., a differentiable physics library which includes simulation environments, interaction potentials, neural networks, and more."
  },
  {
    "name": "Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization. Pranav Subramani, Nicholas Vadivelu, Gautam Kamath. arXiv 2020.",
    "url": "https://arxiv.org/abs/2010.09063",
    "description": "Uses JAX's JIT and VMAP to achieve faster differentially private than existing libraries."
  },
  {
    "name": "Getting started with JAX (MLPs, CNNs & RNNs) by Robert Lange",
    "url": "https://roberttlange.github.io/posts/2020/03/blog-post-10/",
    "description": "Neural network building blocks from scratch with the basic JAX operators."
  },
  {
    "name": "Tutorial: image classification with JAX and Flax Linen by 8bitmp3",
    "url": "https://github.com/8bitmp3/JAX-Flax-Tutorial-Image-Classification-with-Linen",
    "description": "Learn how to create a simple convolutional network with the Linen API by Flax and train it to recognize handwritten digits."
  },
  {
    "name": "Plugging Into JAX by Nick Doiron",
    "url": "https://medium.com/swlh/plugging-into-jax-16c120ec3302",
    "description": "Compares Flax, Haiku, and Objax on the Kaggle flower classification challenge."
  },
  {
    "name": "Meta-Learning in 50 Lines of JAX by Eric Jang",
    "url": "https://blog.evjang.com/2019/02/maml-jax.html",
    "description": "Introduction to both JAX and Meta-Learning."
  },
  {
    "name": "Normalizing Flows in 100 Lines of JAX by Eric Jang",
    "url": "https://blog.evjang.com/2019/07/nf-jax.html",
    "description": "Concise implementation of RealNVP."
  },
  {
    "name": "Differentiable Path Tracing on the GPU/TPU by Eric Jang",
    "url": "https://blog.evjang.com/2019/11/jaxpt.html",
    "description": "Tutorial on implementing path tracing."
  },
  {
    "name": "Ensemble networks by Mat Kelcey",
    "url": "http://matpalm.com/blog/ensemble_nets",
    "description": "Ensemble nets are a method of representing an ensemble of models as one single logical model."
  },
  {
    "name": "Out of distribution (OOD) detection by Mat Kelcey",
    "url": "http://matpalm.com/blog/ood_using_focal_loss",
    "description": "Implements different methods for OOD detection."
  },
  {
    "name": "Understanding Autodiff with JAX by Srihari Radhakrishna",
    "url": "https://www.radx.in/jax.html",
    "description": "Understand how autodiff works using JAX."
  },
  {
    "name": "From PyTorch to JAX: towards neural net frameworks that purify stateful code by Sabrina J. Mielke",
    "url": "https://sjmielke.com/jax-purify.htm",
    "description": "Showcases how to go from a PyTorch-like style of coding to a more Functional-style of coding."
  },
  {
    "name": "Extending JAX with custom C++ and CUDA code by Dan Foreman-Mackey",
    "url": "https://github.com/dfm/extending-jax",
    "description": "Tutorial demonstrating the infrastructure required to provide custom ops in JAX."
  },
  {
    "name": "Evolving Neural Networks in JAX by Robert Tjarko Lange",
    "url": "https://roberttlange.github.io/posts/2021/02/cma-es-jax/",
    "description": "Explores how JAX can power the next generation of scalable neuroevolution algorithms."
  },
  {
    "name": "Exploring hyperparameter meta-loss landscapes with JAX by Luke Metz",
    "url": "http://lukemetz.com/exploring-hyperparameter-meta-loss-landscapes-with-jax/",
    "description": "Demonstrates how to use JAX to perform inner-loss optimization with SGD and Momentum, outer-loss optimization with gradients, and outer-loss optimization using evolutionary strategies."
  },
  {
    "name": "Deterministic ADVI in JAX by Martin Ingram",
    "url": "https://martiningram.github.io/deterministic-advi/",
    "description": "Walk through of implementing automatic differentiation variational inference (ADVI) easily and cleanly with JAX."
  },
  {
    "name": "Evolved channel selection by Mat Kelcey",
    "url": "http://matpalm.com/blog/evolved_channel_selection/",
    "description": "Trains a classification model robust to different combinations of input channels at different resolutions, then uses a genetic algorithm to decide the best combination for a particular loss."
  },
  {
    "name": "Introduction to JAX by Kevin Murphy",
    "url": "https://colab.research.google.com/github/probml/probml-notebooks/blob/main/notebooks/jax_intro.ipynb",
    "description": "Colab that introduces various aspects of the language and applies them to simple ML problems."
  },
  {
    "name": "Writing an MCMC sampler in JAX by Jeremie Coullon",
    "url": "https://www.jeremiecoullon.com/2020/11/10/mcmcjax3ways/",
    "description": "Tutorial on the different ways to write an MCMC sampler in JAX along with speed benchmarks."
  },
  {
    "name": "How to add a progress bar to JAX scans and loops by Jeremie Coullon",
    "url": "https://www.jeremiecoullon.com/2021/01/29/jax_progress_bar/",
    "description": "Tutorial on how to add a progress bar to compiled loops in JAX using the host_callback module."
  },
  {
    "name": "Get started with JAX by Aleksa GordiÄ‡",
    "url": "https://github.com/gordicaleksa/get-started-with-JAX",
    "description": "A series of notebooks and videos going from zero JAX knowledge to building neural networks in Haiku."
  },
  {
    "name": "Reddit",
    "url": "https://www.reddit.com/r/JAX/"
  }
]
