[
  {
    "name": "Apache Hadoop",
    "url": "http://hadoop.apache.org/",
    "description": "Apache Hadoop"
  },
  {
    "name": "Apache Tez",
    "url": "http://tez.apache.org/",
    "description": "A Framework for YARN-based, Data Processing Applications In Hadoop"
  },
  {
    "name": "SpatialHadoop",
    "url": "http://spatialhadoop.cs.umn.edu/",
    "description": "SpatialHadoop is a MapReduce extension to Apache Hadoop designed specially to work with spatial data."
  },
  {
    "name": "GIS Tools for Hadoop",
    "url": "http://esri.github.io/gis-tools-for-hadoop/",
    "description": "Big Data Spatial Analytics for the Hadoop Framework"
  },
  {
    "name": "Elasticsearch Hadoop",
    "url": "https://github.com/elastic/elasticsearch-hadoop",
    "description": "Elasticsearch real-time search and analytics natively integrated with Hadoop. Supports Map/Reduce, Cascading, Apache Hive and Apache Pig."
  },
  {
    "name": "hadoopy",
    "url": "https://github.com/bwhite/hadoopy",
    "description": "Python MapReduce library written in Cython."
  },
  {
    "name": "mrjob",
    "url": "https://github.com/Yelp/mrjob/",
    "description": "mrjob is a Python 2.5+ package that helps you write and run Hadoop Streaming jobs."
  },
  {
    "name": "pydoop",
    "url": "http://pydoop.sourceforge.net/",
    "description": "Pydoop is a package that provides a Python API for Hadoop."
  },
  {
    "name": "hdfs-du",
    "url": "https://github.com/twitter/hdfs-du",
    "description": "HDFS-DU is an interactive visualization of the Hadoop distributed file system."
  },
  {
    "name": "White Elephant",
    "url": "https://github.com/linkedin/white-elephant",
    "description": "Hadoop log aggregator and dashboard"
  },
  {
    "name": "Genie",
    "url": "https://github.com/Netflix/genie",
    "description": "Genie provides REST-ful APIs to run Hadoop, Hive and Pig jobs, and to manage multiple Hadoop resources and perform job submissions across them."
  },
  {
    "name": "Apache Kylin",
    "url": "http://kylin.incubator.apache.org/",
    "description": "Apache Kylin is an open source Distributed Analytics Engine from eBay Inc. that provides SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets"
  },
  {
    "name": "Crunch",
    "url": "https://github.com/jondot/crunch",
    "description": "Go-based toolkit for ETL and feature extraction on Hadoop"
  },
  {
    "name": "Apache Ignite",
    "url": "http://ignite.apache.org/",
    "description": "Distributed in-memory platform"
  },
  {
    "name": "Apache Slider",
    "url": "http://slider.incubator.apache.org/",
    "description": "Apache Slider is a project in incubation at the Apache Software Foundation with the goal of making it possible and easy to deploy existing applications onto a YARN cluster."
  },
  {
    "name": "Apache Twill",
    "url": "http://twill.incubator.apache.org/",
    "description": "Apache Twill is an abstraction over Apache Hadoop® YARN that reduces the complexity of developing distributed applications, allowing developers to focus more on their application logic."
  },
  {
    "name": "mpich2-yarn",
    "url": "https://github.com/alibaba/mpich2-yarn",
    "description": "Running MPICH2 on Yarn"
  },
  {
    "name": "Apache HBase",
    "url": "http://hbase.apache.org",
    "description": "Apache HBase"
  },
  {
    "name": "Apache Phoenix",
    "url": "http://phoenix.apache.org/",
    "description": "A SQL skin over HBase supporting secondary indices"
  },
  {
    "name": "happybase",
    "url": "https://github.com/wbolster/happybase",
    "description": "A developer-friendly Python library to interact with Apache HBase."
  },
  {
    "name": "Hannibal",
    "url": "https://github.com/sentric/hannibal",
    "description": "Hannibal is tool to help monitor and maintain HBase-Clusters that are configured for manual splitting."
  },
  {
    "name": "Haeinsa",
    "url": "https://github.com/VCNC/haeinsa",
    "description": "Haeinsa is linearly scalable multi-row, multi-table transaction library for HBase"
  },
  {
    "name": "hindex",
    "url": "https://github.com/Huawei-Hadoop/hindex",
    "description": "Secondary Index for HBase"
  },
  {
    "name": "Apache Accumulo",
    "url": "https://accumulo.apache.org/",
    "description": "The Apache Accumulo™ sorted, distributed key/value store is a robust, scalable, high performance data storage and retrieval system."
  },
  {
    "name": "OpenTSDB",
    "url": "http://opentsdb.net/",
    "description": "The Scalable Time Series Database"
  },
  {
    "name": "Apache Cassandra",
    "url": "http://cassandra.apache.org/"
  },
  {
    "name": "Apache Hive",
    "url": "http://hive.apache.org",
    "description": "The Apache Hive data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL"
  },
  {
    "name": "Apache Phoenix A SQL skin over HBase supporting secondary indices",
    "url": "http://phoenix.apache.org"
  },
  {
    "name": "Apache HAWQ (incubating)",
    "url": "http://hawq.incubator.apache.org/",
    "description": "Apache HAWQ is a Hadoop native SQL query engine that combines the key technological advantages of MPP database with the scalability and convenience of Hadoop"
  },
  {
    "name": "Lingual",
    "url": "http://www.cascading.org/projects/lingual/",
    "description": "SQL interface for Cascading (MR/Tez job generator)"
  },
  {
    "name": "Apache Impala",
    "url": "https://impala.apache.org/",
    "description": "Apache Impala is an open source massively parallel processing (MPP) SQL query engine for data stored in a computer cluster running Apache Hadoop. Impala has been described as the open-source equivalent of Google F1, which inspired its development in 2012."
  },
  {
    "name": "Presto",
    "url": "https://prestodb.io/",
    "description": "Distributed SQL Query Engine for Big Data. Open sourced by Facebook."
  },
  {
    "name": "Apache Tajo",
    "url": "http://tajo.apache.org/",
    "description": "Data warehouse system for Apache Hadoop"
  },
  {
    "name": "Apache Drill",
    "url": "https://drill.apache.org/",
    "description": "Schema-free SQL Query Engine"
  },
  {
    "name": "Apache Trafodion",
    "url": "http://trafodion.apache.org/"
  },
  {
    "name": "Apache Calcite",
    "url": "http://calcite.apache.org/",
    "description": "A Dynamic Data Management Framework"
  },
  {
    "name": "Apache Atlas",
    "url": "http://atlas.incubator.apache.org/",
    "description": "Metadata tagging & lineage capture suppoting complex business data taxonomies"
  },
  {
    "name": "Apache Kudu",
    "url": "https://kudu.apache.org/",
    "description": "Kudu provides a combination of fast inserts/updates and efficient columnar scans to enable multiple real-time analytic workloads across a single storage layer, complementing HDFS and Apache HBase."
  },
  {
    "name": "Confluent Schema registry for Kafka",
    "url": "https://github.com/confluentinc/schema-registry",
    "description": "Schema Registry provides a serving layer for your metadata. It provides a RESTful interface for storing and retrieving Avro schemas."
  },
  {
    "name": "Hortonworks Schema Registry",
    "url": "https://github.com/hortonworks/registry",
    "description": "Schema Registry is a framework to build metadata repositories."
  },
  {
    "name": "Apache Oozie",
    "url": "http://oozie.apache.org",
    "description": "Apache Oozie"
  },
  {
    "name": "Azkaban",
    "url": "http://azkaban.github.io/"
  },
  {
    "name": "Apache Falcon",
    "url": "http://falcon.apache.org/",
    "description": "Data management and processing platform"
  },
  {
    "name": "Apache NiFi",
    "url": "http://nifi.apache.org/",
    "description": "A dataflow system"
  },
  {
    "name": "Apache AirFlow",
    "url": "https://github.com/apache/incubator-airflow",
    "description": "Airflow is a workflow automation and scheduling system that can be used to author and manage data pipelines"
  },
  {
    "name": "Luigi",
    "url": "http://luigi.readthedocs.org/en/latest/",
    "description": "Python package that helps you build complex pipelines of batch jobs"
  },
  {
    "name": "Apache Flume",
    "url": "http://flume.apache.org",
    "description": "Apache Flume"
  },
  {
    "name": "Suro",
    "url": "https://github.com/Netflix/suro",
    "description": "Netflix's distributed Data Pipeline"
  },
  {
    "name": "Apache Sqoop",
    "url": "http://sqoop.apache.org",
    "description": "Apache Sqoop"
  },
  {
    "name": "Apache Kafka",
    "url": "http://kafka.apache.org/",
    "description": "Apache Kafka"
  },
  {
    "name": "Gobblin from LinkedIn",
    "url": "https://github.com/linkedin/gobblin",
    "description": "Universal data ingestion framework for Hadoop"
  },
  {
    "name": "Apache Pig",
    "url": "http://pig.apache.org",
    "description": "Apache Pig"
  },
  {
    "name": "Apache DataFu",
    "url": "http://datafu.incubator.apache.org/",
    "description": "A collection of libraries for working with large-scale data in Hadoop"
  },
  {
    "name": "vahara",
    "url": "https://github.com/thedatachef/varaha",
    "description": "Machine learning and natural language processing with Apache Pig"
  },
  {
    "name": "packetpig",
    "url": "https://github.com/packetloop/packetpig",
    "description": "Open Source Big Data Security Analytics"
  },
  {
    "name": "akela",
    "url": "https://github.com/mozilla-metrics/akela",
    "description": "Mozilla's utility library for Hadoop, HBase, Pig, etc."
  },
  {
    "name": "seqpig",
    "url": "http://seqpig.sourceforge.net/",
    "description": "Simple and scalable scripting for large sequencing data set(ex: bioinfomation) in Hadoop"
  },
  {
    "name": "Lipstick",
    "url": "https://github.com/Netflix/Lipstick",
    "description": "Pig workflow visualization tool. Introducing Lipstick on A(pache) Pig"
  },
  {
    "name": "PigPen",
    "url": "https://github.com/Netflix/PigPen",
    "description": "PigPen is map-reduce for Clojure, or distributed Clojure. It compiles to Apache Pig, but you don't need to know much about Pig to use it."
  },
  {
    "name": "Kite Software Development Kit",
    "url": "http://kitesdk.org/",
    "description": "A set of libraries, tools, examples, and documentation"
  },
  {
    "name": "gohadoop",
    "url": "https://github.com/hortonworks/gohadoop",
    "description": "Native go clients for Apache Hadoop YARN."
  },
  {
    "name": "Hue",
    "url": "http://gethue.com/",
    "description": "A Web interface for analyzing data with Apache Hadoop."
  },
  {
    "name": "Apache Zeppelin",
    "url": "https://zeppelin.incubator.apache.org/",
    "description": "A web-based notebook that enables interactive data analytics"
  },
  {
    "name": "Jumbune",
    "url": "https://github.com/impetus-opensource/jumbune",
    "description": "Jumbune is an open-source product built for analyzing Hadoop cluster and MapReduce jobs."
  },
  {
    "name": "Apache Thrift",
    "url": "http://thrift.apache.org/"
  },
  {
    "name": "Apache Avro",
    "url": "http://avro.apache.org/",
    "description": "Apache Avro is a data serialization system."
  },
  {
    "name": "Elephant Bird",
    "url": "https://github.com/twitter/elephant-bird",
    "description": "Twitter's collection of LZO and Protocol Buffer-related Hadoop, Pig, Hive, and HBase code."
  },
  {
    "name": "Spring for Apache Hadoop",
    "url": "http://projects.spring.io/spring-hadoop/"
  },
  {
    "name": "hdfs",
    "url": "https://github.com/colinmarc/hdfs",
    "description": "A native go client for HDFS"
  },
  {
    "name": "Oozie Eclipse Plugin",
    "url": "https://marketplace.eclipse.org/content/oozie-eclipse-plugin",
    "description": "A graphical editor for editing Apache Oozie workflows inside Eclipse."
  },
  {
    "name": "Hydrosphere Mist",
    "url": "https://github.com/Hydrospheredata/mist",
    "description": "a service for exposing Apache Spark analytics jobs and machine learning models as realtime, batch or reactive web services."
  },
  {
    "name": "snakebite",
    "url": "https://pypi.python.org/pypi/snakebite/",
    "description": "A pure python HDFS client"
  },
  {
    "name": "Apache Parquet",
    "url": "https://parquet.apache.org/",
    "description": "Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language."
  },
  {
    "name": "Apache Superset (incubating)",
    "url": "https://superset.incubator.apache.org/",
    "description": "Apache Superset (incubating) is a modern, enterprise-ready business intelligence web application"
  },
  {
    "name": "Schema Registry UI",
    "url": "https://github.com/Landoop/schema-registry-ui",
    "description": "Web tool for the Confluent Schema Registry in order to create / view / search / evolve / view history & configure Avro schemas of your Kafka cluster."
  },
  {
    "name": "Apache Storm",
    "url": "http://storm.apache.org/"
  },
  {
    "name": "Apache Samza",
    "url": "http://samza.apache.org/"
  },
  {
    "name": "Apache Spark",
    "url": "http://spark.apache.org/streaming/"
  },
  {
    "name": "Apache Flink",
    "url": "https://flink.apache.org/features.html#unified-stream-amp-batch-processing",
    "description": "Apache Flink is a platform for efficient, distributed, general-purpose data processing. It supports exactly once stream processing."
  },
  {
    "name": "Apache Pulsar (incubating)",
    "url": "http://pulsar.incubator.apache.org/",
    "description": "Apache Pulsar (incubating) is a highly scalable, low latency messaging platform running on commodity hardware. It provides simple pub-sub semantics over topics, guaranteed at-least-once delivery of messages, automatic cursor management for subscribers, and cross-datacenter replication."
  },
  {
    "name": "Druid",
    "url": "http://druid.io/",
    "description": "Column oriented distributed data store ideal for powering interactive applications"
  },
  {
    "name": "Apache Spark",
    "url": "http://spark.apache.org/"
  },
  {
    "name": "Spark Packages",
    "url": "http://spark-packages.org/",
    "description": "A community index of packages for Apache Spark"
  },
  {
    "name": "SparkHub",
    "url": "https://sparkhub.databricks.com/",
    "description": "A community site for Apache Spark"
  },
  {
    "name": "Apache Crunch",
    "url": "http://crunch.apache.org"
  },
  {
    "name": "Cascading",
    "url": "http://www.cascading.org/",
    "description": "Cascading is the proven application development platform for building data applications on Hadoop."
  },
  {
    "name": "Apache Flink",
    "url": "http://flink.apache.org/",
    "description": "Apache Flink is a platform for efficient, distributed, general-purpose data processing."
  },
  {
    "name": "Apache Apex (incubating)",
    "url": "http://apex.incubator.apache.org/",
    "description": "Enterprise-grade unified stream and batch processing engine."
  },
  {
    "name": "Apache Livy (incubating)",
    "url": "https://livy.incubator.apache.org/",
    "description": "Apache Livy (incubating) is web service that exposes a REST interface for managing long running Apache Spark contexts in your cluster. With Livy, new applications can be built on top of Apache Spark that require fine grained interaction with many Spark contexts."
  },
  {
    "name": "Apache Bigtop",
    "url": "http://bigtop.apache.org/",
    "description": "Apache Bigtop: Packaging and tests of the Apache Hadoop ecosystem"
  },
  {
    "name": "Apache Ambari",
    "url": "http://ambari.apache.org/",
    "description": "Apache Ambari"
  },
  {
    "name": "Ganglia Monitoring System",
    "url": "http://ganglia.sourceforge.net/"
  },
  {
    "name": "ankush",
    "url": "https://github.com/impetus-opensource/ankush",
    "description": "A big data cluster management tool that creates and manages clusters of different technologies."
  },
  {
    "name": "Apache Zookeeper",
    "url": "http://zookeeper.apache.org/",
    "description": "Apache Zookeeper"
  },
  {
    "name": "Apache Curator",
    "url": "http://curator.apache.org/",
    "description": "ZooKeeper client wrapper and rich ZooKeeper framework"
  },
  {
    "name": "Buildoop",
    "url": "https://github.com/keedio/buildoop",
    "description": "Hadoop Ecosystem Builder"
  },
  {
    "name": "Deploop",
    "url": "http://deploop.github.io/",
    "description": "The Hadoop Deploy System"
  },
  {
    "name": "Jumbune",
    "url": "http://www.jumbune.org/",
    "description": "An open source MapReduce profiling, MapReduce flow debugging, HDFS data quality validation and Hadoop cluster monitoring tool."
  },
  {
    "name": "inviso",
    "url": "https://github.com/Netflix/inviso",
    "description": "Inviso is a lightweight tool that provides the ability to search for Hadoop jobs, visualize the performance, and view cluster utilization."
  },
  {
    "name": "ElasticSearch",
    "url": "https://www.elastic.co/"
  },
  {
    "name": "Apache Solr",
    "url": "http://lucene.apache.org/solr/",
    "description": "Apache Solr is an open source search platform built upon a Java library called Lucene."
  },
  {
    "name": "SenseiDB",
    "url": "http://www.senseidb.com/",
    "description": "Open-source, distributed, realtime, semi-structured database"
  },
  {
    "name": "Banana",
    "url": "https://github.com/LucidWorks/banana",
    "description": "Kibana port for Apache Solr"
  },
  {
    "name": "Apache Nutch",
    "url": "http://nutch.apache.org/",
    "description": "Apache Nutch is a highly extensible and scalable open source web crawler software project."
  },
  {
    "name": "Apache Ranger",
    "url": "http://ranger.incubator.apache.org/",
    "description": "Ranger is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform."
  },
  {
    "name": "Apache Sentry",
    "url": "https://sentry.incubator.apache.org/",
    "description": "An authorization module for Hadoop"
  },
  {
    "name": "Apache Knox Gateway",
    "url": "https://knox.apache.org/",
    "description": "A REST API Gateway for interacting with Hadoop clusters."
  },
  {
    "name": "Project Rhino",
    "url": "https://github.com/intel-hadoop/project-rhino",
    "description": "Intel's open source effort to enhance the existing data protection capabilities of the Hadoop ecosystem to address security and compliance challenges, and contribute the code back to Apache."
  },
  {
    "name": "Big Data Benchmark",
    "url": "https://amplab.cs.berkeley.edu/benchmark/"
  },
  {
    "name": "HiBench",
    "url": "https://github.com/intel-hadoop/HiBench"
  },
  {
    "name": "Big-Bench",
    "url": "https://github.com/intel-hadoop/Big-Data-Benchmark-for-Big-Bench"
  },
  {
    "name": "hive-benchmarks",
    "url": "https://github.com/yhuai/hive-benchmarks"
  },
  {
    "name": "hive-testbench",
    "url": "https://github.com/cartershanklin/hive-testbench",
    "description": "Testbench for experimenting with Apache Hive at any data scale."
  },
  {
    "name": "YCSB",
    "url": "https://github.com/brianfrankcooper/YCSB",
    "description": "The Yahoo! Cloud Serving Benchmark (YCSB) is an open-source specification and program suite for evaluating retrieval and maintenance capabilities of computer programs. It is often used to compare relative performance of NoSQL database management systems."
  },
  {
    "name": "Apache Mahout",
    "url": "http://mahout.apache.org"
  },
  {
    "name": "Oryx 2",
    "url": "https://github.com/OryxProject/oryx",
    "description": "Lambda architecture on Spark, Kafka for real-time large scale machine learning"
  },
  {
    "name": "MLlib",
    "url": "https://spark.apache.org/mllib/",
    "description": "MLlib is Apache Spark's scalable machine learning library."
  },
  {
    "name": "R",
    "url": "http://www.r-project.org/",
    "description": "R is a free software environment for statistical computing and graphics."
  },
  {
    "name": "RHadoop including RHDFS, RHBase, RMR2, plyrmr",
    "url": "https://github.com/RevolutionAnalytics/RHadoop/wiki"
  },
  {
    "name": "RHive RHive, for launching Hive queries from R",
    "url": "https://github.com/nexr/RHive"
  },
  {
    "name": "Apache Lens",
    "url": "http://lens.apache.org/"
  },
  {
    "name": "Apache SINGA (incubating)",
    "url": "https://singa.incubator.apache.org/",
    "description": "SINGA is a general distributed deep learning platform for training big deep learning models over large datasets"
  },
  {
    "name": "BigDL",
    "url": "https://bigdl-project.github.io/",
    "description": "BigDL is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters."
  },
  {
    "name": "http://nexr.github.io/hive-udf/",
    "url": "http://nexr.github.io/hive-udf/"
  },
  {
    "name": "https://github.com/edwardcapriolo/hive_cassandra_udfs",
    "url": "https://github.com/edwardcapriolo/hive_cassandra_udfs"
  },
  {
    "name": "https://github.com/livingsocial/HiveSwarm",
    "url": "https://github.com/livingsocial/HiveSwarm"
  },
  {
    "name": "https://github.com/ThinkBigAnalytics/Hive-Extensions-from-Think-Big-Analytics",
    "url": "https://github.com/ThinkBigAnalytics/Hive-Extensions-from-Think-Big-Analytics"
  },
  {
    "name": "https://github.com/karthkk/udfs",
    "url": "https://github.com/karthkk/udfs"
  },
  {
    "name": "https://github.com/twitter/elephant-bird",
    "url": "https://github.com/twitter/elephant-bird",
    "description": "Twitter"
  },
  {
    "name": "https://github.com/lovelysystems/ls-hive",
    "url": "https://github.com/lovelysystems/ls-hive"
  },
  {
    "name": "https://github.com/stewi2/hive-udfs",
    "url": "https://github.com/stewi2/hive-udfs"
  },
  {
    "name": "https://github.com/klout/brickhouse",
    "url": "https://github.com/klout/brickhouse"
  },
  {
    "name": "https://github.com/markgrover/hive-translate (PostgreSQL translate())",
    "url": "https://github.com/markgrover/hive-translate"
  },
  {
    "name": "https://github.com/deanwampler/HiveUDFs",
    "url": "https://github.com/deanwampler/HiveUDFs"
  },
  {
    "name": "https://github.com/myui/hivemall (Machine Learning UDF/UDAF/UDTF)",
    "url": "https://github.com/myui/hivemall"
  },
  {
    "name": "https://github.com/edwardcapriolo/hive-geoip (GeoIP UDF)",
    "url": "https://github.com/edwardcapriolo/hive-geoip"
  },
  {
    "name": "https://github.com/Netflix/Surus",
    "url": "https://github.com/Netflix/Surus"
  },
  {
    "name": "https://github.com/dvasilen/Hive-Cassandra",
    "url": "https://github.com/dvasilen/Hive-Cassandra"
  },
  {
    "name": "https://github.com/yc-huang/Hive-mongo",
    "url": "https://github.com/yc-huang/Hive-mongo"
  },
  {
    "name": "https://github.com/balshor/gdata-storagehandler",
    "url": "https://github.com/balshor/gdata-storagehandler"
  },
  {
    "name": "https://bitbucket.org/rodrigopr/redisstoragehandler",
    "url": "https://bitbucket.org/rodrigopr/redisstoragehandler"
  },
  {
    "name": "https://github.com/zhuguangbin/HiveJDBCStorageHanlder",
    "url": "https://github.com/zhuguangbin/HiveJDBCStorageHanlder"
  },
  {
    "name": "https://github.com/chimpler/hive-solr",
    "url": "https://github.com/chimpler/hive-solr"
  },
  {
    "name": "https://github.com/bfemiano/accumulo-hive-storage-manager",
    "url": "https://github.com/bfemiano/accumulo-hive-storage-manager"
  },
  {
    "name": "https://github.com/rcongiu/Hive-JSON-Serde",
    "url": "https://github.com/rcongiu/Hive-JSON-Serde"
  },
  {
    "name": "https://github.com/mochi/hive-json-serde",
    "url": "https://github.com/mochi/hive-json-serde"
  },
  {
    "name": "https://github.com/ogrodnek/csv-serde",
    "url": "https://github.com/ogrodnek/csv-serde"
  },
  {
    "name": "https://github.com/parag/HiveJsonSerde",
    "url": "https://github.com/parag/HiveJsonSerde"
  },
  {
    "name": "https://github.com/forward3d/rbhive",
    "url": "https://github.com/forward3d/rbhive"
  },
  {
    "name": "https://github.com/synctree/activerecord-hive-adapter",
    "url": "https://github.com/synctree/activerecord-hive-adapter"
  },
  {
    "name": "https://github.com/hrp/sequel-hive-adapter",
    "url": "https://github.com/hrp/sequel-hive-adapter"
  },
  {
    "name": "https://github.com/forward/node-hive",
    "url": "https://github.com/forward/node-hive"
  },
  {
    "name": "https://github.com/recruitcojp/WebHive",
    "url": "https://github.com/recruitcojp/WebHive"
  },
  {
    "name": "shib",
    "url": "https://github.com/tagomoris/shib",
    "description": "WebUI for query engines: Hive and Presto"
  },
  {
    "name": "clive",
    "url": "https://github.com/bmuller/clive",
    "description": "Clojure library for interacting with Hive via Thrift"
  },
  {
    "name": "https://github.com/dmorel/Thrift-API-HiveClient2 (Perl",
    "url": "https://github.com/dmorel/Thrift-API-HiveClient2",
    "description": "HiveServer2)"
  },
  {
    "name": "PyHive",
    "url": "https://github.com/dropbox/PyHive",
    "description": "Python interface to Hive and Presto"
  },
  {
    "name": "https://github.com/recruitcojp/OdbcHive",
    "url": "https://github.com/recruitcojp/OdbcHive"
  },
  {
    "name": "Hive-Sharp",
    "url": "https://bitbucket.org/vadim/hive-sharp"
  },
  {
    "name": "HiveRunner",
    "url": "https://github.com/klarna/HiveRunner",
    "description": "An Open Source unit test framework for hadoop hive queries based on JUnit4"
  },
  {
    "name": "Beetest",
    "url": "https://github.com/kawaa/Beetest",
    "description": "A super simple utility for testing Apache Hive scripts locally for non-Java developers."
  },
  {
    "name": "Hive_test- Unit test framework for hive and hive-service",
    "url": "https://github.com/edwardcapriolo/hive_test"
  },
  {
    "name": "Flume MongoDB Sink",
    "url": "https://github.com/leonlee/flume-ng-mongodb-sink"
  },
  {
    "name": "Flume HornetQ Channel",
    "url": "https://github.com/btoddb/flume-ng-hornetq-channel"
  },
  {
    "name": "Flume MessagePack Source",
    "url": "https://github.com/leonlee/flume-ng-msgpack-source"
  },
  {
    "name": "Flume RabbitMQ source and sink",
    "url": "https://github.com/jcustenborder/flume-ng-rabbitmq"
  },
  {
    "name": "Flume UDP Source",
    "url": "https://github.com/whitepages/flume-udp-source"
  },
  {
    "name": "Stratio Ingestion",
    "url": "https://github.com/Stratio/Ingestion",
    "description": "Custom sinks: Cassandra, MongoDB, Stratio Streaming and JDBC"
  },
  {
    "name": "Flume Custom Serializers",
    "url": "https://github.com/relistan/flume-serializers"
  },
  {
    "name": "Real-time analytics in Apache Flume",
    "url": "https://github.com/jrkinley/flume-interceptor-analytics"
  },
  {
    "name": ".Net FlumeNG Clients",
    "url": "https://github.com/marksl/DotNetFlumeNG.Clients"
  },
  {
    "name": "Hadoop Weekly",
    "url": "http://www.hadoopweekly.com/"
  },
  {
    "name": "The Hadoop Ecosystem Table",
    "url": "http://hadoopecosystemtable.github.io/"
  },
  {
    "name": "Hadoop 1.x vs 2",
    "url": "http://www.slideshare.net/RommelGarcia2/hadoop-1x-vs-2"
  },
  {
    "name": "Apache Hadoop YARN: Yet Another Resource Negotiator",
    "url": "http://www.socc2013.org/home/program/a5-vavilapalli.pdf"
  },
  {
    "name": "Introducing Apache Hadoop YARN",
    "url": "http://hortonworks.com/blog/introducing-apache-hadoop-yarn/"
  },
  {
    "name": "Programming Community Curated Resources for Learning Hadoop",
    "url": "https://hackr.io/tutorials/learn-hadoop-big-data"
  },
  {
    "name": "Apache Hadoop YARN",
    "url": "http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/",
    "description": "Background and an Overview"
  },
  {
    "name": "Apache Hadoop YARN",
    "url": "http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/",
    "description": "Concepts and Applications"
  },
  {
    "name": "Apache Hadoop YARN",
    "url": "http://hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/",
    "description": "ResourceManager"
  },
  {
    "name": "Apache Hadoop YARN",
    "url": "http://hortonworks.com/blog/apache-hadoop-yarn-nodemanager/",
    "description": "NodeManager"
  },
  {
    "name": "Migrating to MapReduce 2 on YARN (For Users)",
    "url": "http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-users/"
  },
  {
    "name": "Migrating to MapReduce 2 on YARN (For Operators)",
    "url": "http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-operators/"
  },
  {
    "name": "Hadoop and Big Data: Use Cases at Salesforce.com",
    "url": "https://developer.salesforce.com/blogs/engineering/2013/03/hadoop-use-cases-at-salesforce-com.html"
  },
  {
    "name": "All you wanted to know about Hadoop, but were too afraid to ask: genealogy of elephants.",
    "url": "https://blogs.apache.org/bigtop/entry/all_you_wanted_to_know"
  },
  {
    "name": "What is Bigtop, and Why Should You Care?",
    "url": "https://blogs.apache.org/bigtop/entry/bigtop_and_why_should_you"
  },
  {
    "name": "Hadoop",
    "url": "http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support",
    "description": "Distributions and Commercial Support"
  },
  {
    "name": "Ganglia configuration for a small Hadoop cluster and some troubleshooting",
    "url": "http://hakunamapdata.com/ganglia-configuration-for-a-small-hadoop-cluster-and-some-troubleshooting/"
  },
  {
    "name": "Hadoop illuminated",
    "url": "http://hadoopilluminated.com/",
    "description": "Open Source Hadoop Book"
  },
  {
    "name": "NoSQL Database",
    "url": "http://nosql-database.org/"
  },
  {
    "name": "10 Best Practices for Apache Hive",
    "url": "https://www.qubole.com/blog/big-data/hive-best-practices/"
  },
  {
    "name": "Hadoop Operations at Scale",
    "url": "http://hortonworks.com/blog/apache-hadoop-operations-scale/"
  },
  {
    "name": "AWS BigData Blog",
    "url": "http://blogs.aws.amazon.com/bigdata/"
  },
  {
    "name": "Hadoop360",
    "url": "http://www.hadoop360.com/"
  },
  {
    "name": "How to monitor Hadoop metrics",
    "url": "https://www.datadoghq.com/blog/monitor-hadoop-metrics/"
  },
  {
    "name": "Hadoop Summit Presentations",
    "url": "http://www.slideshare.net/Hadoop_Summit",
    "description": "Slide decks from Hadoop Summit"
  },
  {
    "name": "Hadoop 24/7",
    "url": "http://www.slideshare.net/allenwittenauer/aw-apachecon2009-15342917"
  },
  {
    "name": "An example Apache Hadoop Yarn upgrade",
    "url": "http://www.slideshare.net/mikejf12/an-example-apache-hadoop-yarn-upgrade"
  },
  {
    "name": "Apache Hadoop In Theory And Practice",
    "url": "http://www.slideshare.net/AdamKawa/hadoop-intheoryandpractice"
  },
  {
    "name": "Hadoop Operations at LinkedIn",
    "url": "http://www.slideshare.net/allenwittenauer/2013-hadoopsummitemea"
  },
  {
    "name": "Hadoop Performance at LinkedIn",
    "url": "http://www.slideshare.net/allenwittenauer/2012-lihadoopperf"
  },
  {
    "name": "Docker based Hadoop provisioning",
    "url": "http://www.slideshare.net/JanosMatyas/docker-based-hadoop-provisioning"
  },
  {
    "name": "Hadoop: The Definitive Guide",
    "url": "http://www.amazon.com/gp/product/1449311520/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=1449311520&linkCode=as2&tag=matratsblo-20"
  },
  {
    "name": "Hadoop Operations",
    "url": "http://www.amazon.com/gp/product/1449327052/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=1449327052&linkCode=as2&tag=matratsblo-20"
  },
  {
    "name": "Apache Hadoop Yarn",
    "url": "http://www.amazon.com/dp/0321934504?tag=matratsblo-20"
  },
  {
    "name": "HBase: The Definitive Guide",
    "url": "http://shop.oreilly.com/product/0636920014348.do"
  },
  {
    "name": "Programming Pig",
    "url": "http://shop.oreilly.com/product/0636920018087.do"
  },
  {
    "name": "Programming Hive",
    "url": "http://shop.oreilly.com/product/0636920023555.do"
  },
  {
    "name": "Hadoop in Practice, Second Edition",
    "url": "http://www.manning.com/holmes2/"
  },
  {
    "name": "Hadoop in Action, Second Edition",
    "url": "http://www.manning.com/lam2/"
  },
  {
    "name": "ApacheCon",
    "url": "http://www.apachecon.com/"
  },
  {
    "name": "Strata + Hadoop World",
    "url": "http://conferences.oreilly.com/strata"
  },
  {
    "name": "DataWorks Summit",
    "url": "https://dataworkssummit.com/"
  },
  {
    "name": "Spark Summit",
    "url": "https://databricks.com/sparkaisummit"
  }
]
